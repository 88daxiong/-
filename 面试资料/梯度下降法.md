# 梯度下降法的三种形式

## 0x1 批量梯度下降（BGD）

批量梯度下降法（Batch Gradient Descent, BGD）是梯度下降的最原始方法，**核心点在于每次都使用所有的样本来更新参数**，最小化损失函数，如果只有一个极小值，那么BGD是考虑了所有的数据，朝着代价最小的方向运动，缺点在于**如果样本值很大的话，更新速度很慢**。
$$
\theta = \theta - \eta.\nabla J(\theta)
$$
其中，$$J(\theta)$$为整个数据集上的代价函数，$$\theta$$为我们要更新的参数；

## 0x2 随机梯度下降（SGD）

随机梯度下降（Stochastic Gradient Descent, SGD）,**使用每个样本的损失函数来更新参数**。

正是由于只考虑了一个样本点，所以会大大的加快训练数据，也就是弥补了BGD训练速度慢的问题。然而，如果训练数据中噪声点太多的话，那么每一次利用噪声点进行更新的过程中，就不一定是朝着极小值的方向更新，但由于更新多轮，整体方向上还是超着极小值的方向更新，又提高了速度。
$$
\theta = \theta - \eta.\nabla_{\theta} J(\theta; x^i;y^i)
$$
这里，就是只是根据其中的某一个样本i更新参数；

## 0x3 小批量梯度下降（MBGD）

小批量梯度下降（Mini-batch Gradient Descent, MBGD）,为了解决BGD的训练慢和SGD的准确率的问题，提出了MBGD，**每次使用一个batch的数量进行更新参数**，不同的问题batch是不一样的。
$$
\theta = \theta - \eta.\nabla_{\theta}J(\theta;x^{i:i+n};y^{i:i+n})
$$
这里，是根据i-i+n之间的样本来更新参数的；

需要注意的是，这三种梯度学习方法都是使用的相同的学习率，需要预先设置学习率，而且整个计算的过程中不改变，这里会产生如下的问题：

1）：选择一个合适的学习率是非常困难的事情。学习率较小，收敛速度将会非常慢；而学习率较大时，收敛过程将会变得非常抖动，而且有可能不能收敛到最优。

2）：预先制定学习率变化规则。比如，计算30轮之后，学习率减半。但是这种方式需要预先定义学习率变化的规则，而规则的准确率在训练过程中并不能保证。

3）：上述三种算法针对所有数据采用相同的学习速率，但是当我们的数据非常稀疏的时候，我们可能不希望所有数据都以相同的方式进行梯度更新，而是对这种极少的特征进行一次大的更新。

4）：高度非凸函数普遍出现在神经网络中，在优化这类函数时，另一个关键的挑战是使函数避免陷入无数次优的局部最小值。



参考：https://www.jianshu.com/p/0acd30a23e4e